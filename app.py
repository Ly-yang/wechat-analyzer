# app.py - Flaskåç«¯æœåŠ¡å™¨
from flask import Flask, request, jsonify
from flask_cors import CORS
from pymongo import MongoClient
from celery import Celery
import requests
import re
import jieba
import jieba.analyse
from datetime import datetime, timedelta
import hashlib
import json
import os
from bs4 import BeautifulSoup
import schedule
import time
from threading import Thread
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# MongoDBé…ç½®
MONGO_URI = os.getenv('MONGO_URI', 'mongodb://localhost:27017/')
client = MongoClient(MONGO_URI)
db = client.wechat_analyzer

# Celeryé…ç½®
app.config['CELERY_BROKER_URL'] = 'redis://localhost:6379/0'
app.config['CELERY_RESULT_BACKEND'] = 'redis://localhost:6379/0'

celery = Celery(app.name, broker=app.config['CELERY_BROKER_URL'])
celery.conf.update(app.config)

# å¾®ä¿¡å…¬ä¼—å·é‡‡é›†å™¨
class WeChatCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def search_articles(self, keyword, category='all', days=7):
        """æœç´¢å¾®ä¿¡æ–‡ç« """
        try:
            # è¿™é‡Œéœ€è¦æ¥å…¥å¾®ä¿¡å…¬ä¼—å·APIæˆ–ç¬¬ä¸‰æ–¹æœåŠ¡
            # ç”±äºå¾®ä¿¡é™åˆ¶ï¼Œè¿™é‡Œæä¾›æ¨¡æ‹Ÿæ•°æ®ç»“æ„
            mock_articles = self._generate_mock_articles(keyword, category)
            return mock_articles
        except Exception as e:
            logger.error(f"æœç´¢æ–‡ç« å¤±è´¥: {e}")
            return []
    
    def _generate_mock_articles(self, keyword, category):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ–‡ç« æ•°æ®"""
        import random
        
        titles = [
            f"ğŸ”¥ {keyword}å¿…çœ‹æŒ‡å—ï¼š10ä¸ªè®©ä½ æ•ˆç‡ç¿»å€çš„æŠ€å·§",
            f"ğŸ’¡ å…³äº{keyword}ï¼Œ99%çš„äººéƒ½ä¸çŸ¥é“çš„ç§˜å¯†",
            f"ğŸ“ˆ {keyword}èµšé’±æ”»ç•¥ï¼šä»0åˆ°æœˆå…¥è¿‡ä¸‡çš„å®Œæ•´è·¯å¾„",
            f"ğŸ¯ {keyword}é«˜æ‰‹éƒ½åœ¨ç”¨çš„5ä¸ªæ ¸å¿ƒæ–¹æ³•",
            f"ğŸ’° {keyword}å˜ç°æŒ‡å—ï¼šæ™®é€šäººä¹Ÿèƒ½å®ç°è´¢åŠ¡è‡ªç”±",
            f"ğŸš€ {keyword}æ–°æ‰‹å…¥é—¨ï¼š30å¤©ä»å°ç™½åˆ°ä¸“å®¶",
            f"âš¡ {keyword}æ•ˆç‡æå‡ï¼šè¿™äº›å·¥å…·è®©ä½ äº‹åŠåŠŸå€",
            f"ğŸ§  {keyword}æ€ç»´å‡çº§ï¼šæ”¹å˜è®¤çŸ¥ï¼Œæ”¹å˜äººç”Ÿ"
        ]
        
        authors = ["è´¢å¯Œè‡ªç”±ä¹‹è·¯", "æ•ˆç‡å·¥å…·ç®±", "èŒåœºè¿›é˜¶æŒ‡å—", "åˆ›ä¸šé‚¦", "ç†è´¢å°è¾¾äºº", "æˆé•¿ç¬”è®°", "æŠ•èµ„æ€ç»´", "æƒ…å•†å­¦é™¢"]
        
        articles = []
        for i in range(8):
            article = {
                'id': hashlib.md5(f"{titles[i]}{i}".encode()).hexdigest(),
                'title': titles[i],
                'author': random.choice(authors),
                'content_url': f"https://mp.weixin.qq.com/s/{hashlib.md5(titles[i].encode()).hexdigest()}",
                'read_count': random.randint(50000, 500000),
                'like_count': random.randint(1000, 20000),
                'publish_time': datetime.now() - timedelta(days=random.randint(1, 7)),
                'category': category if category != 'all' else random.choice(['tech', 'finance', 'lifestyle', 'education']),
                'summary': f"è¿™æ˜¯ä¸€ç¯‡å…³äº{keyword}çš„æ·±åº¦æ–‡ç« ï¼Œé€šè¿‡å®é™…æ¡ˆä¾‹å’Œæ•°æ®åˆ†æï¼Œä¸ºè¯»è€…æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œå»ºè®®ã€‚",
                'keywords': [keyword] + [f"å…³é”®è¯{j}" for j in range(3)],
                'crawl_time': datetime.now()
            }
            articles.append(article)
        
        return articles
    
    def extract_content(self, url):
        """æå–æ–‡ç« å†…å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–æ–‡ç« å†…å®¹
            content_div = soup.find('div', class_='rich_media_content')
            if content_div:
                return content_div.get_text(strip=True)
            return ""
        except Exception as e:
            logger.error(f"æå–å†…å®¹å¤±è´¥: {e}")
            return ""

# æ–‡ç« åˆ†æå™¨
class ArticleAnalyzer:
    def __init__(self):
        # åˆå§‹åŒ–jiebaåˆ†è¯
        jieba.initialize()
        
        # æƒ…æ„Ÿè¯å…¸
        self.emotion_words = {
            'positive': ['çˆ†æ¬¾', 'å¿…çœ‹', 'å¹²è´§', 'ç»æ‹›', 'ç§˜å¯†', 'æŠ€å·§', 'æ”»ç•¥', 'é€†è¢­', 'æˆåŠŸ', 'èµšé’±'],
            'negative': ['å¤±è´¥', 'é”™è¯¯', 'é™·é˜±', 'éª—å±€', 'äºæŸ', 'å±é™©'],
            'urgent': ['ç´§æ€¥', 'ç«‹å³', 'é©¬ä¸Š', 'èµ¶ç´§', 'å¿«é€Ÿ', 'æ€¥éœ€'],
            'exclusive': ['ç‹¬å®¶', 'é¦–å‘', 'å†…éƒ¨', 'ä¸“å±', 'é™æ—¶', 'ç¨€æœ‰']
        }
    
    def analyze_article(self, title, content=""):
        """åˆ†ææ–‡ç« """
        analysis = {
            'structure': self._analyze_structure(title, content),
            'emotion': self._analyze_emotion(title, content),
            'engagement': self._analyze_engagement(title, content),
            'seo': self._analyze_seo(title, content),
            'keywords': self._extract_keywords(title, content),
            'score': 0
        }
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        analysis['score'] = self._calculate_score(analysis)
        return analysis
    
    def _analyze_structure(self, title, content):
        """ç»“æ„åˆ†æ"""
        return {
            'title_length': len(title),
            'content_length': len(content),
            'sentence_count': len(re.findall(r'[ã€‚ï¼ï¼Ÿ.!?]', content)),
            'paragraph_count': len(content.split('\n\n')) if content else 0,
            'avg_sentence_length': len(content) / max(len(re.findall(r'[ã€‚ï¼ï¼Ÿ.!?]', content)), 1) if content else 0
        }
    
    def _analyze_emotion(self, title, content):
        """æƒ…æ„Ÿåˆ†æ"""
        text = title + content
        emotion_scores = {}
        
        for emotion_type, words in self.emotion_words.items():
            score = sum(1 for word in words if word in text)
            emotion_scores[emotion_type] = score
        
        total_emotion_words = sum(emotion_scores.values())
        emotion_intensity = (total_emotion_words / max(len(text), 1)) * 100
        
        return {
            'emotion_scores': emotion_scores,
            'emotion_intensity': round(emotion_intensity, 2),
            'dominant_emotion': max(emotion_scores, key=emotion_scores.get) if emotion_scores else None
        }
    
    def _analyze_engagement(self, title, content):
        """äº’åŠ¨æ€§åˆ†æ"""
        text = title + content
        
        return {
            'question_count': len(re.findall(r'[ï¼Ÿ?]', text)),
            'exclamation_count': len(re.findall(r'[ï¼!]', text)),
            'number_count': len(re.findall(r'\d+', text)),
            'call_to_action': len(re.findall(r'(ç‚¹èµ|è½¬å‘|å…³æ³¨|æ”¶è—|åˆ†äº«)', text))
        }
    
    def _analyze_seo(self, title, content):
        """SEOåˆ†æ"""
        return {
            'title_length_optimal': 15 <= len(title) <= 30,
            'keyword_in_title': True,  # ç®€åŒ–å¤„ç†
            'readability_score': 85,  # æ¨¡æ‹Ÿå¯è¯»æ€§è¯„åˆ†
            'keyword_density': 3.2   # æ¨¡æ‹Ÿå…³é”®è¯å¯†åº¦
        }
    
    def _extract_keywords(self, title, content):
        """æå–å…³é”®è¯"""
        text = title + content
        keywords = jieba.analyse.extract_tags(text, topK=10, withWeight=True)
        return [{'word': word, 'weight': weight} for word, weight in keywords]
    
    def _calculate_score(self, analysis):
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        score = 0
        
        # ç»“æ„è¯„åˆ† (30%)
        if 15 <= analysis['structure']['title_length'] <= 30:
            score += 30
        
        # æƒ…æ„Ÿè¯„åˆ† (25%)
        if analysis['emotion']['emotion_intensity'] > 5:
            score += 25
        
        # äº’åŠ¨è¯„åˆ† (25%)
        engagement_total = (analysis['engagement']['question_count'] + 
                           analysis['engagement']['exclamation_count'] + 
                           analysis['engagement']['call_to_action'])
        if engagement_total >= 3:
            score += 25
        
        # SEOè¯„åˆ† (20%)
        if analysis['seo']['title_length_optimal']:
            score += 20
        
        return min(score, 100)

# æ¨¡æ¿ç”Ÿæˆå™¨
class TemplateGenerator:
    def __init__(self):
        self.templates = {
            'listicle': {
                'name': 'æ¸…å•å‹æ–‡ç« ',
                'title_formula': '{æ•°å­—} + {æ ¸å¿ƒè¯} + {äººç¾¤} + {è·å¾—æ„Ÿ}',
                'structure': [
                    'æ ‡é¢˜ï¼šä½¿ç”¨å…·ä½“æ•°å­— + æ ¸å¿ƒå…³é”®è¯ + ç›®æ ‡äººç¾¤ + ä»·å€¼æ‰¿è¯º',
                    'å¼€å¤´ï¼šç°è±¡æè¿° + ç—›ç‚¹æŒ–æ˜ + è§£å†³æ–¹æ¡ˆé¢„å‘Š',
                    'æ­£æ–‡ï¼šNä¸ªè¦ç‚¹ï¼Œæ¯ä¸ªè¦ç‚¹åŒ…å«è§£é‡Šã€æ¡ˆä¾‹ã€æ•°æ®æ”¯æ’‘',
                    'ç»“å°¾ï¼šæ€»ç»“è¦ç‚¹ + è¡ŒåŠ¨å·å¬ + äº’åŠ¨å¼•å¯¼'
                ],
                'examples': [
                    'èŒåœºäººå¿…å¤‡ï¼š5ä¸ªè®©ä½ åŠ è–ª30%çš„æ ¸å¿ƒæŠ€èƒ½',
                    'ç†è´¢å°ç™½æŒ‡å—ï¼š7ä¸ªè®©ä½ è´¢å¯Œç¿»å€çš„æŠ•èµ„æ–¹æ³•'
                ]
            },
            'howto': {
                'name': 'æ•™ç¨‹æŒ‡å—',
                'title_formula': 'å¦‚ä½•/æ€æ · + {å…·ä½“ç›®æ ‡} + {æ—¶é—´/æ•°é‡}',
                'structure': [
                    'æ ‡é¢˜ï¼šæ˜ç¡®çš„how-toæ ¼å¼ + å…·ä½“å¯é‡åŒ–çš„ç›®æ ‡',
                    'å¼€å¤´ï¼šé—®é¢˜èƒŒæ™¯ + æ–¹æ³•é¢„è§ˆ + ä»·å€¼æ‰¿è¯º',
                    'æ­£æ–‡ï¼šæ­¥éª¤åˆ†è§£ + è¯¦ç»†è¯´æ˜ + æ³¨æ„äº‹é¡¹ + å·¥å…·æ¨è',
                    'ç»“å°¾ï¼šæ•ˆæœé¢„æœŸ + å®è·µå»ºè®® + è¿›é˜¶å­¦ä¹ èµ„æº'
                ],
                'examples': [
                    'å¦‚ä½•åœ¨30å¤©å†…æŒæ¡Pythonç¼–ç¨‹ï¼šé›¶åŸºç¡€å®Œæ•´æŒ‡å—',
                    'æ€æ ·ç”¨100å…ƒå¼€å§‹ç†è´¢ï¼šæ–°æ‰‹æŠ•èµ„å®æ“æ‰‹å†Œ'
                ]
            }
        }
    
    def generate_template(self, article_type, audience, keywords):
        """ç”Ÿæˆå†™ä½œæ¨¡æ¿"""
        template = self.templates.get(article_type, self.templates['listicle'])
        
        # æ ¹æ®å—ä¼—å’Œå…³é”®è¯å®šåˆ¶æ¨¡æ¿
        customized = {
            'type': template['name'],
            'audience': audience,
            'keywords': keywords.split(',') if keywords else [],
            'title_formula': template['title_formula'],
            'structure': template['structure'],
            'examples': template['examples'],
            'tips': self._generate_tips(article_type, audience),
            'generated_time': datetime.now()
        }
        
        return customized
    
    def _generate_tips(self, article_type, audience):
        """ç”Ÿæˆå†™ä½œæŠ€å·§"""
        base_tips = [
            'æ ‡é¢˜ä¸­ä½¿ç”¨å…·ä½“æ•°å­—å¢åŠ å¯ä¿¡åº¦',
            'å¼€å¤´3å¥è¯å†…æŠ“ä½è¯»è€…æ³¨æ„åŠ›',
            'æ¯æ®µä¸è¶…è¿‡3å¥è¯ï¼Œä¿æŒå¯è¯»æ€§',
            'é€‚å½“ä½¿ç”¨emojiå¢åŠ è§†è§‰å¸å¼•åŠ›',
            'ç»“å°¾è¦æœ‰æ˜ç¡®çš„è¡ŒåŠ¨æŒ‡å¼•'
        ]
        
        audience_tips = {
            'youth': ['ä½¿ç”¨å¹´è½»äººç†Ÿæ‚‰çš„ç½‘ç»œç”¨è¯­', 'å¤šç”¨æµè¡Œæ¢—å’Œè¡¨æƒ…åŒ…'],
            'professional': ['æä¾›å®ç”¨çš„èŒåœºæ¡ˆä¾‹', 'å¼•ç”¨æƒå¨æ•°æ®å’Œç ”ç©¶'],
            'parent': ['å…³æ³¨è‚²å„¿å’Œå®¶åº­åœºæ™¯', 'æä¾›å®ç”¨çš„ç”Ÿæ´»æŠ€å·§'],
            'entrepreneur': ['åˆ†äº«åˆ›ä¸šæ•…äº‹å’Œç»éªŒ', 'æä¾›å•†ä¸šæ€ç»´å’Œæ–¹æ³•'],
            'senior': ['è¯­è¨€é€šä¿—æ˜“æ‡‚', 'å¤šç”¨ä¼ ç»Ÿä»·å€¼è§‚å¿µ']
        }
        
        return base_tips + audience_tips.get(audience, [])

# å®ä¾‹åŒ–æ ¸å¿ƒç»„ä»¶
crawler = WeChatCrawler()
analyzer = ArticleAnalyzer()
template_generator = TemplateGenerator()

# Celeryä»»åŠ¡
@celery.task
def crawl_articles_task(keywords, category, min_reads):
    """å¼‚æ­¥é‡‡é›†ä»»åŠ¡"""
    try:
        articles = []
        for keyword in keywords:
            keyword_articles = crawler.search_articles(keyword, category)
            # è¿‡æ»¤é˜…è¯»é‡
            filtered_articles = [a for a in keyword_articles if a['read_count'] >= min_reads]
            articles.extend(filtered_articles)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        for article in articles:
            db.articles.update_one(
                {'id': article['id']},
                {'$set': article},
                upsert=True
            )
        
        logger.info(f"é‡‡é›†å®Œæˆï¼Œå…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
        return len(articles)
    
    except Exception as e:
        logger.error(f"é‡‡é›†ä»»åŠ¡å¤±è´¥: {e}")
        return 0

# APIè·¯ç”±
@app.route('/api/articles/collect', methods=['POST'])
def collect_articles():
    """å¼€å§‹é‡‡é›†æ–‡ç« """
    try:
        data = request.json
        keywords = data.get('keywords', ['çƒ­é—¨']).split(',') if isinstance(data.get('keywords'), str) else data.get('keywords', ['çƒ­é—¨'])
        category = data.get('category', 'all')
        min_reads = int(data.get('min_reads', 10000))
        
        # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
        task = crawl_articles_task.delay(keywords, category, min_reads)
        
        return jsonify({
            'status': 'success',
            'task_id': task.id,
            'message': 'é‡‡é›†ä»»åŠ¡å·²å¯åŠ¨'
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/articles', methods=['GET'])
def get_articles():
    """è·å–æ–‡ç« åˆ—è¡¨"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        category = request.args.get('category', 'all')
        limit = int(request.args.get('limit', 20))
        offset = int(request.args.get('offset', 0))
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {}
        if category != 'all':
            query['category'] = category
        
        # æŸ¥è¯¢æ•°æ®åº“
        articles = list(db.articles.find(query)
                       .sort('crawl_time', -1)
                       .skip(offset)
                       .limit(limit))
        
        # è½¬æ¢ObjectIdä¸ºå­—ç¬¦ä¸²
        for article in articles:
            article['_id'] = str(article['_id'])
            if isinstance(article.get('publish_time'), datetime):
                article['publish_time'] = article['publish_time'].isoformat()
            if isinstance(article.get('crawl_time'), datetime):
                article['crawl_time'] = article['crawl_time'].isoformat()
        
        total = db.articles.count_documents(query)
        
        return jsonify({
            'status': 'success',
            'data': articles,
            'total': total,
            'limit': limit,
            'offset': offset
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/articles/<article_id>/analyze', methods=['POST'])
def analyze_article_endpoint(article_id):
    """åˆ†ææŒ‡å®šæ–‡ç« """
    try:
        # ä»æ•°æ®åº“è·å–æ–‡ç« 
        article = db.articles.find_one({'id': article_id})
        if not article:
            return jsonify({
                'status': 'error',
                'message': 'æ–‡ç« æœªæ‰¾åˆ°'
            }), 404
        
        # è·å–æ–‡ç« å†…å®¹
        content = ""
        if article.get('content_url'):
            content = crawler.extract_content(article['content_url'])
        
        # æ‰§è¡Œåˆ†æ
        analysis = analyzer.analyze_article(article['title'], content)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_record = {
            'article_id': article_id,
            'analysis': analysis,
            'analyze_time': datetime.now()
        }
        
        db.analysis.update_one(
            {'article_id': article_id},
            {'$set': analysis_record},
            upsert=True
        )
        
        return jsonify({
            'status': 'success',
            'data': analysis
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/analyze/text', methods=['POST'])
def analyze_text():
    """åˆ†æè‡ªå®šä¹‰æ–‡æœ¬"""
    try:
        data = request.json
        title = data.get('title', '')
        content = data.get('content', '')
        
        if not title and not content:
            return jsonify({
                'status': 'error',
                'message': 'è¯·æä¾›æ ‡é¢˜æˆ–å†…å®¹'
            }), 400
        
        analysis = analyzer.analyze_article(title, content)
        
        return jsonify({
            'status': 'success',
            'data': analysis
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/templates/generate', methods=['POST'])
def generate_template():
    """ç”Ÿæˆå†™ä½œæ¨¡æ¿"""
    try:
        data = request.json
        article_type = data.get('type', 'listicle')
        audience = data.get('audience', 'professional')
        keywords = data.get('keywords', '')
        
        template = template_generator.generate_template(article_type, audience, keywords)
        
        # ä¿å­˜æ¨¡æ¿
        template_record = {
            'template': template,
            'user_id': data.get('user_id', 'anonymous'),
            'created_time': datetime.now()
        }
        
        result = db.templates.insert_one(template_record)
        template['template_id'] = str(result.inserted_id)
        
        return jsonify({
            'status': 'success',
            'data': template
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/templates', methods=['GET'])
def get_templates():
    """è·å–æ¨¡æ¿åˆ—è¡¨"""
    try:
        user_id = request.args.get('user_id', 'anonymous')
        limit = int(request.args.get('limit', 10))
        
        templates = list(db.templates.find({'user_id': user_id})
                        .sort('created_time', -1)
                        .limit(limit))
        
        for template in templates:
            template['_id'] = str(template['_id'])
            if isinstance(template.get('created_time'), datetime):
                template['created_time'] = template['created_time'].isoformat()
        
        return jsonify({
            'status': 'success',
            'data': templates
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/stats/dashboard', methods=['GET'])
def get_dashboard_stats():
    """è·å–ä»ªè¡¨æ¿ç»Ÿè®¡æ•°æ®"""
    try:
        # ç»Ÿè®¡æ•°æ®
        total_articles = db.articles.count_documents({})
        total_analysis = db.analysis.count_documents({})
        total_templates = db.templates.count_documents({})
        
        # æœ€è¿‘7å¤©çš„æ–‡ç« æ•°é‡è¶‹åŠ¿
        seven_days_ago = datetime.now() - timedelta(days=7)
        recent_articles = list(db.articles.aggregate([
            {'$match': {'crawl_time': {'$gte': seven_days_ago}}},
            {'$group': {
                '_id': {'$dateToString': {'format': '%Y-%m-%d', 'date': '$crawl_time'}},
                'count': {'$sum': 1}
            }},
            {'$sort': {'_id': 1}}
        ]))
        
        # åˆ†ç±»ç»Ÿè®¡
        category_stats = list(db.articles.aggregate([
            {'$group': {'_id': '$category', 'count': {'$sum': 1}}},
            {'$sort': {'count': -1}}
        ]))
        
        # çƒ­é—¨å…³é”®è¯
        hot_keywords = list(db.articles.aggregate([
            {'$unwind': '$keywords'},
            {'$group': {'_id': '$keywords', 'count': {'$sum': 1}}},
            {'$sort': {'count': -1}},
            {'$limit': 10}
        ]))
        
        return jsonify({
            'status': 'success',
            'data': {
                'total_articles': total_articles,
                'total_analysis': total_analysis,
                'total_templates': total_templates,
                'article_trend': recent_articles,
                'category_stats': category_stats,
                'hot_keywords': hot_keywords
            }
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0'
    })

# å®šæ—¶ä»»åŠ¡
def run_scheduled_tasks():
    """è¿è¡Œå®šæ—¶ä»»åŠ¡"""
    def job():
        logger.info("æ‰§è¡Œå®šæ—¶é‡‡é›†ä»»åŠ¡")
        crawl_articles_task.delay(['çƒ­é—¨', 'çˆ†æ¬¾', 'å¹²è´§'], 'all', 10000)
    
    schedule.every().hour.do(job)
    
    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == '__main__':
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡çº¿ç¨‹
    scheduler_thread = Thread(target=run_scheduled_tasks)
    scheduler_thread.daemon = True
    scheduler_thread.start()
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(debug=True, host='0.0.0.0', port=5000)
